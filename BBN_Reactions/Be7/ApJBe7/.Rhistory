from scipy import special
def   drumhead_height(n, k, distance, angle, t):
kth_zero = special.jn_zeros(n, k)[-1]
return
np.cos(t) * np.cos(n*angle)*special.jn(n, distance*kth_zero)
from scipy.special import logsumexp
a = np.arange(10)
np.log(np.sum(np.exp(a)))
from scipy.special import logsumexp
a = np.arange(10)
from galpy.potential import MiyamotoNagaiPotential
mp= MiyamotoNagaiPotential(a=0.5,b=0.0375,normalize=1.)
mp.plotRotcurve(Rrange=[0.01,10.],grid=1001)
library(keras)
use_condaenv("r-tensorflow")
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
require(ggplot2)
# autoencoder in keras
suppressPackageStartupMessages(library(keras))
ais <- iris[,1:4]
# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais, 2, minmax)
# set training data
x_train <- as.matrix(x_train)
# set model
model <- keras_model_sequential()
model %>%
layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
layer_dense(units = 2, activation = "tanh", name = "bottleneck") %>%
layer_dense(units = 6, activation = "tanh") %>%
layer_dense(units = ncol(x_train))
# view model layers
summary(model)
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# fit model
model2 <-  model %>% fit(
x = x_train,
y = x_train,
epochs = 5000,
verbose = 0
)
# evaluate the performance of the model
mse.ae2 <- evaluate(model, x_train, x_train)
mse.ae2
intermediate_layer_model <- keras_model(inputs = model$input, outputs = get_layer(model, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
ggplot(data.frame(PC1 = intermediate_output[,1], PC2 = intermediate_output[,2]), aes(x = PC1, y = PC2, col = iris$Species)) + geom_point()
setwd("~/Documents/GitHub/coinFR2019_supernovae/Rafael")
require(ggplot2)
# autoencoder in keras
suppressPackageStartupMessages(library(keras))
ais <- iris[,1:4]
# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais, 2, minmax)
# set training data
x_train <- as.matrix(x_train)
# set model
model <- keras_model_sequential()
model %>%
layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
layer_dense(units = 2, activation = "tanh", name = "bottleneck") %>%
layer_dense(units = 6, activation = "tanh") %>%
layer_dense(units = ncol(x_train))
# view model layers
summary(model)
model %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# fit model
model2 <-  model %>% fit(
x = x_train,
y = x_train,
epochs = 5000,
verbose = 0
)
# evaluate the performance of the model
mse.ae2 <- evaluate(model, x_train, x_train)
mse.ae2
intermediate_layer_model <- keras_model(inputs = model$input, outputs = get_layer(model, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
ggplot(data.frame(PC1 = intermediate_output[,1], PC2 = intermediate_output[,2]), aes(x = PC1, y = PC2, col = iris$Species)) + geom_point()
quit()
